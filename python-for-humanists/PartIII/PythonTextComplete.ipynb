{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of text analysis\n",
    "\n",
    "* Word frequency\n",
    "* Word associations (collocates)\n",
    "* Tracking words over time\n",
    "\n",
    "### [Quantitative Analysis of Culture Using Millions of Digitized Books](https://science.sciencemag.org/content/331/6014/176)\n",
    "* Using a corpus of 5 million English language books from 1800-200, all which were digitized by Google books.\n",
    "\n",
    "![](flu.png)\n",
    "![](greatwar.png)\n",
    "![](evolution.png)\n",
    "\n",
    "### Yale Daily News Corpus\n",
    "* Digitized YDN from 1878-1996.\n",
    "* Looking at terms around coeducation.\n",
    "    * Yale College became co-ed in 1969.\n",
    "    * Subset of corpus looking at 1959-1979.\n",
    "    \n",
    "![](coed.png)\n",
    "![](man_woman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of text data\n",
    "\n",
    "* The easiest method for working with textual data in python is to use plain-text files.\n",
    "    * Common plain-text files include:\n",
    "        * .txt\n",
    "        * .xml\n",
    "        * .csv\n",
    "        * .json\n",
    "        * .html\n",
    "        * .tsv\n",
    "        * more...\n",
    "* Word Docs & Excel spreadsheets are not plain-text files.\n",
    "\n",
    "### Creating raw data\n",
    "* The first step in most text based research will be creating text files. These files can come from a variety of sources:\n",
    "    * Archival documents\n",
    "    * Government documents\n",
    "    * Web pages\n",
    "    * Newspapers & periodicals\n",
    "    * Twiiter\n",
    "* It's common to start with a PDF or other scan of a physical work.\n",
    "* **Optical Character Recognition** (OCR) is the process of extracting text from an image file or PDF.\n",
    "    * OCR can create plain-text files that can be used in many types of analysis.\n",
    "* This workshop will not cover OCR, but we can recommend software for OCR:\n",
    "    * ABBYY FInereader\n",
    "    * Adobe Acrobat\n",
    "    * Python-tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing text files\n",
    "### Opening text files\n",
    "\n",
    "* There are a few methods for opening files in python. We wil focus on using the built-in python function `open()`. \n",
    "    * Other libraries, like `pandas` and `NLTK` have their own functions for reading text files. \n",
    "* `dir()` will list every method or function available to call upon an object.\n",
    "* `.read()` will read the contents of a text file. We can print the reasult or save the read text to another variable.\n",
    "* `.readlines()` will split our file into a list. Each item in this list is one line of our text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('./rawText/obama_speeches_000.txt','r')\n",
    "print(dir(infile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = infile.readline()\n",
    "date = infile.readline()\n",
    "text = infile.read()\n",
    "print(title, date, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sub-Strings\n",
    "* Python lets us slice strings based on the index of each character\n",
    "    * We use square brackets `[:]` to designate the dimensions of a slice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title[8:-3]\n",
    "date = date[7:-3]\n",
    "\n",
    "print(title,date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning strings\n",
    "* We can slice our `title` and `date` varibales to remove erroneous characters.\n",
    "* The `replace()` function will remove spaces from the speech title and reaplce them with an underscore.\n",
    "    * It's best practice not to have spaces in a filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title.replace(' ','_')\n",
    "\n",
    "print(title,date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing files\n",
    "\n",
    "* The `open()` function is also used to write new files.\n",
    "* The new filename and path is set in the first arguement.\n",
    "    * If the file does not exist, python will create it.\n",
    "    * If the file akready exists, python will overwrite the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('./cleanText/' + date + '_' + title + '.txt', 'w')\n",
    "outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning files in batches\n",
    "\n",
    "* The power of python is more apparent when working with large amounts of data.\n",
    "* The code above works to clean one text file in a collection.\n",
    "    * By adding this code to a `for` loop, python can batch process our text files.\n",
    "* `glob.glob` creates a list of filenames based on a pattern.\n",
    "* The `.close()` method will close the file before moving on to the next file in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "filenames = glob.glob('./rawText/*.txt')\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as infile:\n",
    "        title = infile.readline().replace(' ','_')[8:-3]\n",
    "        date = infile.readline()[7:-3]\n",
    "        text = infile.read()\n",
    "        outfile = open('./cleanText/' + title + '.txt', 'w')\n",
    "        outfile.write(text)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with XML\n",
    "### Converting XML to TXT using BeautifulSoup\n",
    "* The `BeautifulSoup` library contains functions for working with XML & HTML files.\n",
    "    * `BeautifulSoup` is commonly used when web-scraping in python to parse HTML.\n",
    "* Using `with open() as :` automatically closes the file after the indented lines are complete. Meaning the `.close()` method is not necessary.\n",
    "* BeautifulSoup will read our file object and return parsed XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('./NYHT_raw/1322266375.xml','r') as infile:\n",
    "    soup = BeautifulSoup(infile.read(), 'xml')\n",
    "    print(type(soup))\n",
    "    print(dir(soup))\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find XML Elements by Name\n",
    "* The `soup` variable conatains the XML contents of the file\n",
    "* The `.find()` funciton searches the XML for an element name that matches the arguement string.\n",
    "    * `find()` only matches the first instance of the element.\n",
    "    * Use `find_all()` to return a list of *all* matching elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = soup.find('NumericPubDate').text\n",
    "recordid = soup.find('RecordID').text\n",
    "print(date, recordid)\n",
    "\n",
    "new_file = date + '_' + recordid + '.txt'\n",
    "print(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find('FullText').text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a new file\n",
    "* Python can write our new file to a new directory with our clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./NYHT_clean/' + new_file, 'w') as outfile:\n",
    "    outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch convert XML files to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filenames = glob.glob('./NYHT_raw/*.xml')\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename,'r') as infile:\n",
    "        soup = BeautifulSoup(infile.read(), 'xml')\n",
    "        date = soup.find('NumericPubDate').text\n",
    "        text = soup.find('FullText').text\n",
    "        recordid = soup.find('RecordID').text\n",
    "        with open('./NYHT_clean/' + date + '_' + recordid + '.txt', 'w') as outfile:\n",
    "            outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Latin Text\n",
    "\n",
    "utf-8 encoding allows us to work with scripts outside of the latin alphabet. See the full Unicode character list here: [all unicode characters](https://www.utf8-chartable.de/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = open('./rawText/anna_k.txt', encoding='utf-8')\n",
    "print(rus.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in string functions\n",
    "* Python includes many built-in [string methods](https://docs.python.org/3/library/stdtypes.html#string-methods) helpful in editing text data.\n",
    "* In order to compare words, it's best to standardize them into once case.\n",
    "    * The `lower()` method will convert all characters in a string to lowercase.\n",
    "    * `strip()` by default will remove leading and trailing whitespace (spaces & tabs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('./NYHT_clean/19520926_1322266375.txt', 'r')\n",
    "\n",
    "text = infile.read().lower()\n",
    "print(text)\n",
    "\n",
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom list of special charcters\n",
    "\n",
    "* The `string` library inclides pre-build strings containing letters, numbers, and punctuation.\n",
    "* The `list()` function breaks a string into a list of characters.\n",
    "* The `.replace()` function will replace one string with another. In this case, we replace each special charcter in our list with an empty string. This is equivalent to deleting the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "special = list(string.punctuation)\n",
    "print(special)\n",
    "\n",
    "for x in special:\n",
    "    text = text.replace(x,'')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "* Stopwords is a term given to any words in a corpus that should be ignored in the analysis.\n",
    "* Generally, stopwords are common words in a language that do not give insight into the text.\n",
    "    * This includes words like: *the, and, or, a, an, if,* and many more.\n",
    "* Libraries like `NLTK` will provide built-in stopwords\n",
    "    * The code below manually creates a dictionary of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {'the', 'do', 'in', 'can', \"aren't\", 'needn', \"mightn't\", 'it', 'you', 'through', 'her', 'against', 'because', 'into', \"hasn't\", 'down', 'why', \"don't\", 'again', 'than', \"you're\", 'themselves', 'does', 'weren', 'this', 'll', 'don', 'they', \"didn't\", 'as', 'there', 'too', 'off', \"haven't\", \"mustn't\", 'shan', 'all', 'such', 're', 'have', 'that', 'over', \"it's\", 'most', 'those', 'and', 'yours', \"couldn't\", 'under', 'didn', 'so', 'myself', 'itself', 'herself', \"that'll\", 'theirs', 'should', 'how', 'am', \"you've\", 'be', 'will', 'd', \"needn't\", 'i', 'is', 'couldn', 'or', 'himself', 'other', 'your', 'below', 'our', 'by', 'before', 'between', 'just', 'hasn', 'ain', \"hadn't\", 'not', 'did', 'his', 'shouldn', \"wasn't\", 'to', \"isn't\", 'wouldn', 'haven', 'wasn', 'ma', 'some', 'a', 'their', 'them', 'up', 'few', 'yourselves', 'doesn', 'm', 't', 'at', 'during', 'then', 'when', 'me', 'while', 'aren', 'more', 'isn', 'we', 'above', 'after', 'where', 'until', 'whom', 'o', \"shan't\", 'once', 'being', 'him', 'hers', 'an', 's', 'but', 'further', 'ours', 'both', \"you'll\", 'from', 'own', 'nor', 'who', 'been', \"wouldn't\", 'very', \"won't\", 'on', 'same', 'no', 'doing', 'what', 'she', 'mightn', 'ourselves', 'if', \"should've\", 'hadn', 'was', 'of', \"you'd\", 'are', \"she's\", 'yourself', 'out', 'he', 'mustn', \"doesn't\", 'which', 'having', \"weren't\", 'won', 'for', 'its', 'had', 'has', 'about', 'here', 'only', 'y', 'now', \"shouldn't\", 'my', 'each', 'were', 'with', 'these', 've', 'any'}\n",
    "print(stopwords)\n",
    "\n",
    "\n",
    "for stop in stopwords:\n",
    "    text = text.replace(' '+stop+' ',' ')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a word list\n",
    "\n",
    "* `split()` will split our string into a list of strings. This method splits the string based on the arguement of the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = text.split()\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning individual words\n",
    "* The methods `.isalpha()`, `.isnumeric()`, `.isalnum()`, `.isascii()` return True if *all* characters in the string match the criteria.\n",
    "    * `.isalpha()` returns True if there is at least one character & all characters are letters (latin alphabet).\n",
    "    * `.isnumeric()` returns True if there is at least one character & all characters are digits.  \n",
    "    * `.isalnum()` returns True if there is at least one character & all characters are letters or digits.\n",
    "    * `.isascii()` returns True if all characters are ascii letters, digits, or special characters. See [ASCII Table](https://www.cs.cmu.edu/~pattis/15-1XX/common/handouts/ascii.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ['abc','123','!?-','a2?']\n",
    "\n",
    "for x in w:\n",
    "    print(x, x.isalpha(), x.isnumeric(), x.isalnum(), x.isascii()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word.isalpha() == True:\n",
    "        final_words.append(word)\n",
    "\n",
    "print(final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little fun with ASCII\n",
    "* Run the cell below to make some ASCII art.\n",
    "* We are using an image calle 'logo.jog'. Change the image name or path to convert a differnt image into \"art\".\n",
    "* [Source](https://www.geeksforgeeks.org/converting-image-ascii-image-python/)\n",
    "![Handome Dan Yale logo](./logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = 'python ./ascii_art.py --file logo.jpg --cols 120'\n",
    "os.system(cmd)\n",
    "open('out.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words\n",
    "\n",
    "### Word frequency\n",
    "* The `Counter` library is an easy method for counting unique items in a list.\n",
    "    * We can use counter on our `final_words` list to find word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pets = [\"dog\", \"cat\", \"bird\", \"gnu\", \"dog\", \"dog\", \"cat\"]\n",
    "\n",
    "pet_counts = Counter(pets)\n",
    "print(pet_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(final_words)\n",
    "\n",
    "print(word_counts)\n",
    "print(len(word_counts))\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our results\n",
    "* Our `word_dict` can be saved as a JSON file.\n",
    "    * python dictioanries follow the JSON structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "word_dict = dict(word_counts)\n",
    "print(word_dict)\n",
    "\n",
    "with open('./output/19511216_1322285142.json', 'w') as outfile:\n",
    "    json.dump(word_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also save our results as a CSV file.\n",
    "* A CSV file can be used by many programs like excel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./output/19511216_1322285142.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['word','count'])\n",
    "    for word in word_dict.keys():\n",
    "        row = [word, word_dict[word]]\n",
    "        print(row)\n",
    "        w.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working at Scale\n",
    "\n",
    "### Word frequencies over time\n",
    "\n",
    "* Our goal is to end up with one file containing word frequencies grouped by the article date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {'the', 'do', 'in', 'can', \"aren't\", 'needn', \"mightn't\", 'it', 'you', 'through', 'her', 'against', 'because', 'into', \"hasn't\", 'down', 'why', \"don't\", 'again', 'than', \"you're\", 'themselves', 'does', 'weren', 'this', 'll', 'don', 'they', \"didn't\", 'as', 'there', 'too', 'off', \"haven't\", \"mustn't\", 'shan', 'all', 'such', 're', 'have', 'that', 'over', \"it's\", 'most', 'those', 'and', 'yours', \"couldn't\", 'under', 'didn', 'so', 'myself', 'itself', 'herself', \"that'll\", 'theirs', 'should', 'how', 'am', \"you've\", 'be', 'will', 'd', \"needn't\", 'i', 'is', 'couldn', 'or', 'himself', 'other', 'your', 'below', 'our', 'by', 'before', 'between', 'just', 'hasn', 'ain', \"hadn't\", 'not', 'did', 'his', 'shouldn', \"wasn't\", 'to', \"isn't\", 'wouldn', 'haven', 'wasn', 'ma', 'some', 'a', 'their', 'them', 'up', 'few', 'yourselves', 'doesn', 'm', 't', 'at', 'during', 'then', 'when', 'me', 'while', 'aren', 'more', 'isn', 'we', 'above', 'after', 'where', 'until', 'whom', 'o', \"shan't\", 'once', 'being', 'him', 'hers', 'an', 's', 'but', 'further', 'ours', 'both', \"you'll\", 'from', 'own', 'nor', 'who', 'been', \"wouldn't\", 'very', \"won't\", 'on', 'same', 'no', 'doing', 'what', 'she', 'mightn', 'ourselves', 'if', \"should've\", 'hadn', 'was', 'of', \"you'd\", 'are', \"she's\", 'yourself', 'out', 'he', 'mustn', \"doesn't\", 'which', 'having', \"weren't\", 'won', 'for', 'its', 'had', 'has', 'about', 'here', 'only', 'y', 'now', \"shouldn't\", 'my', 'each', 'were', 'with', 'these', 've', 'any'}\n",
    "punct = {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~'}\n",
    "digits = ['0','1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, glob\n",
    "from collections import Counter\n",
    "\n",
    "cbd = open('./output/counts_by_date.csv', 'w')\n",
    "w = csv.writer(cbd)\n",
    "w.writerow(['word','count','date'])\n",
    "for filename in glob.glob('./NYHT_clean/*.txt'):\n",
    "    with open(filename, 'r') as infile:\n",
    "        text = infile.read().lower()\n",
    "        for p in punct:\n",
    "            text = text.replace(p,'')\n",
    "        for d in digits:\n",
    "            text = text.replace(d,'')\n",
    "        for stop in stopwords:\n",
    "            text = text.replace(' '+stop+' ',' ')\n",
    "        words_by_file = Counter(text.split())\n",
    "        word_dict = dict(words_by_file)\n",
    "        for key in word_dict.keys():\n",
    "            row = [key, word_dict[key], filename[13:17]]\n",
    "            print(row)\n",
    "            w.writerow(row)\n",
    "cbd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total word counts\n",
    "* Now let's work with all of our newspaper articles.\n",
    "* We can use much of the same code as above, but we need to structure it in a loop to work on each file.\n",
    "* Our goal is to produce a single file, with the total count of each word across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, glob\n",
    "from collections import Counter\n",
    "\n",
    "total_words = Counter()\n",
    "\n",
    "for filename in glob.glob('./NYHT_clean/*.txt'):\n",
    "    with open(filename, 'r') as infile:\n",
    "        text = infile.read().lower()\n",
    "        for stop in stopwords:\n",
    "            text = text.replace(' '+stop+' ',' ')\n",
    "        for p in punct:\n",
    "            text = text.replace(p,'')\n",
    "        for d in digits:\n",
    "            text = text.replace(d,'')\n",
    "        total_words.update(text.split())\n",
    "            \n",
    "print(total_words)\n",
    "\n",
    "with open('./output/total_counts.csv', 'w') as outfile:\n",
    "    w = csv.writer(outfile)\n",
    "    w.writerow(['word','count'])\n",
    "    word_dict = dict(total_words)\n",
    "    for key in word_dict.keys():\n",
    "        row = [key, word_dict[key]]\n",
    "        print(row)\n",
    "        w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, glob, pickle\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for filename in glob.glob('./NYHT_clean/*.txt'):\n",
    "    with open(filename, 'r') as infile:\n",
    "        text = infile.read().lower()\n",
    "        for stop in stopwords:\n",
    "            text = text.replace(' '+stop+' ',' ')\n",
    "        for p in punct:\n",
    "            text = text.replace(p,'')\n",
    "        for d in digits:\n",
    "            text = text.replace(d,'')\n",
    "        c.update(text.split())\n",
    "c = dict(c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NYHT.pickle', 'wb') as outfile:\n",
    "    pickle.dump(c,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NYHT.pickle', 'rb') as infile:\n",
    "    counts = pickle.load(infile)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Data\n",
    "\n",
    "### Plotting basics\n",
    "* `matplotlib` is a frequently used graphing library for python.\n",
    "* The most basic plot requires data for the x and y axis.\n",
    "    * `plt.plot()` takes two or more arguements. The data for the two axis is the minimal requirement.\n",
    "    * The axis labels and title are set using methods `.xlabel()`, `.ylabel()`, and `.title()`.\n",
    "* Many other style and graph options are possible with `matplotlib`. See the full documentation: [Matplotlib User Guide](https://matplotlib.org/3.1.0/users/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "year = [91,92,93,94,95,96,97,98,99]\n",
    "count = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "plt.plot(year, count)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Frequency of Word x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./output/counts_by_date.csv', index_col='date')\n",
    "totals = df.groupby('word').sum().sort_values(by=['count'],ascending=False)\n",
    "print(totals,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrs = df[df.word == 'mrs'].groupby('date').sum()\n",
    "mrs.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = totals.loc[['mrs','mr','dr','prof']]\n",
    "terms.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals.plot.hist(figsize=(20,5), bins=[x for x in range(0,200,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting word frequencies\n",
    "* There are many ways to plot word frequencies. The code below loads in a JSON file with word frequencies of certain terms from Yale Daily News from 1959-1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = open('YDN_counts.json','r')\n",
    "count = json.load(data)\n",
    "type(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(count['(gender|sex)'].keys())\n",
    "y = list(count['(gender|sex)'].values())\n",
    "#print(x,y)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x,y,'g--')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of \"gender\" and \"sex\" in Yale Daily News (1959-1979)')\n",
    "plt.savefig('YDN_gender.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = list(count['(coed|coeducation)'].keys())\n",
    "y2 = list(count['(coed|coeducation)'].values())\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x,y,'g--', label = \"SEX\")\n",
    "plt.plot(x2,y2,'b', label = \"COED\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Words in Yale Daily News (1959-1979)')\n",
    "plt.savefig('YDN_terms.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Tools\n",
    "\n",
    "### Working with Voyant\n",
    "* Now that we have text files, we can take our data to other software.\n",
    "* [Voyant](https://voyant-tools.org/) is a free, online tool for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates\n",
    "\n",
    "* The `datetime` library in python adds functions for working with dates.\n",
    "* It can convert strings to ISO dates and vice-versa:\n",
    "    * `strptime()` converts a string date into an ISO date object\n",
    "    * `strftime()` converts a ISO date object to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = 'November 8, 2008'\n",
    "\n",
    "isoDate = datetime.strptime(date, '%B %d, %Y')\n",
    "print(isoDate, type(isoDate))\n",
    "\n",
    "strDate = isoDate.strftime('%Y%m%d')\n",
    "print(isoDate, strDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
